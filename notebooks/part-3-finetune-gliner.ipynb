{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook copied from here:\n",
    "# https://github.com/urchade/GLiNER/blob/main/examples/finetune.ipynb\n",
    "\n",
    "import json\n",
    "from gliner import GLiNER\n",
    "import spacy\n",
    "from gliner_spacy.pipeline import GlinerSpacy\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function loads jsonl data\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NER Dataset for Fine-Tuning\n",
    "\n",
    "**TODO**: you need to load your own NER dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(969, 969, 31, 31, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = 'data/argilla_dataset_train.jsonl'\n",
    "train_data_str_path = 'data/argilla_dataset_train_set_str.jsonl'\n",
    "eval_path = 'data/argilla_dataset_eval.jsonl'\n",
    "eval_data_str_path = 'data/argilla_dataset_eval_set_str.jsonl'\n",
    "baseline_predictions_path = 'data/argilla_dataset_baseline_preds.jsonl'\n",
    "\n",
    "train_data = read_jsonl(train_path)\n",
    "train_data_str = read_jsonl(train_data_str_path)\n",
    "eval_data = read_jsonl(eval_path)\n",
    "eval_data_str = read_jsonl(eval_data_str_path)\n",
    "baseline_predictions = read_jsonl(baseline_predictions_path)\n",
    "\n",
    "len(train_data), len(train_data_str), len(eval_data), len(eval_data_str), len(baseline_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenized_text': ['Dress', ',', 'Shoes', ',', 'and', 'Scarf', 'provided', 'by', 'ModCloth', '.'], 'ner': [[0, 5, 'clothing'], [7, 12, 'clothing'], [18, 23, 'clothing'], [36, 44, 'organization']]}\n",
      "\n",
      "{'tokenized_text': ['You', 'can', 'bring', 'some', 'of', 'varying', 'weight', 'to', 'give', 'yourself', 'options', '.', 'Gaiters', 'â€“', 'these', 'will', 'protect', 'you', 'and', 'your', 'boots', 'from', 'rain', 'or', 'muck', '.'], 'ner': [[105, 110, 'clothing'], [63, 70, 'organization']]}\n"
     ]
    }
   ],
   "source": [
    "# checking for gliner format\n",
    "# {'tokenized_text' [], 'ner': [ [start_token_i, end_token_i, label], ...], ...}\n",
    "print(f\"{train_data[0]}\\n\\n{eval_data[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Trained GLiNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1163d0c3ce24eb5904153c1542ef820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egemenipek/miniconda3/envs/zero-shot-ner-lab/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# available models: https://huggingface.co/urchade\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_largev2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "from gliner import GLiNERConfig, GLiNER\n",
    "from gliner.training import Trainer, TrainingArguments\n",
    "from gliner.data_processing.collator import DataCollatorWithPadding, DataCollator\n",
    "from gliner.utils import load_config_as_namespace\n",
    "from gliner.data_processing import WordsSplitter, GLiNERDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a27b1c64e434b8d87a099b747aa254a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: mps:0\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # if you have apple m-series \n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # if you have gpu\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # you most likely have this :)\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_largev2\")\n",
    "\n",
    "# dynamic padding used to speed training up and save memory\n",
    "data_collator = DataCollator(model.config, data_processor=model.data_processor, prepare_labels=True)\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model is on: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epochs: 2, Number of Batches: 60, Number of Steps: 120\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2  # Define how many times you want to pass over the dataset\n",
    "batch_size = 16\n",
    "data_size = len(train_data)\n",
    "num_batches = data_size // batch_size  # Total batches per epoch\n",
    "num_steps = num_epochs * num_batches  # Total training steps\n",
    "print(f\"Number of Epochs: {num_epochs}, Number of Batches: {num_batches}, Number of Steps: {num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egemenipek/miniconda3/envs/zero-shot-ner-lab/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    others_lr=1e-5,\n",
    "    others_weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\", #cosine\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    focal_loss_alpha=0.75,\n",
    "    focal_loss_gamma=2,\n",
    "    num_train_epochs=num_epochs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    #save_steps = 100,\n",
    "    #save_total_limit=10,\n",
    "    dataloader_num_workers = 0,\n",
    "    use_cpu = False,\n",
    "    #report_to=\"none\",\n",
    "    )\n",
    "\n",
    "# this is to track loss during training\n",
    "class LossTrackerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []  # Store loss per step\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "            print(f\"Step {state.global_step}: Loss {logs['loss']}\")\n",
    "\n",
    "loss_tracker = LossTrackerCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c_/w5by8m2s30jcjd665yxczwx40000gp/T/ipykernel_72830/3698432713.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='122' max='122' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [122/122 02:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=122, training_loss=9.800071841380635, metrics={'train_runtime': 153.0584, 'train_samples_per_second': 12.662, 'train_steps_per_second': 0.797, 'total_flos': 0.0, 'train_loss': 9.800071841380635, 'epoch': 2.0})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    tokenizer=model.data_processor.transformer_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[loss_tracker]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/fine-tuned-gliner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in /Users/egemenipek/zero-shot-ner-fine-tuning-lab/notebooks/models/fine-tuned-gliner\n"
     ]
    }
   ],
   "source": [
    "# possibly, remove from this cell onward from this section\n",
    "fine_tuned_gliner = GLiNER.from_pretrained(\"models/fine-tuned-gliner\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "text = 'dress is a clotthing item. Look at that jacket yo!'\n",
    "\n",
    "labels = ['clothing', 'organization', 'address', 'event']\n",
    "\n",
    "entities = fine_tuned_gliner.predict_entities(text, labels)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in train_data_str:\n",
    "    entities = fine_tuned_gliner.predict_entities(example, labels)\n",
    "    if entities:\n",
    "        print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Predict, Evaluate and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nervaluate import Evaluator\n",
    "\n",
    "# below function converts the data to the format that nervaluate expects\n",
    "def convert_data_to_nervaluate_format(data):\n",
    "    formatted_data = []\n",
    "    for data_point in data:\n",
    "        formatted_data_point = [{'label': ner_point[2], 'start': ner_point[0], 'end': ner_point[1]} for ner_point in data_point['ner']]\n",
    "        formatted_data.append(formatted_data_point)\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in /Users/egemenipek/zero-shot-ner-fine-tuning-lab/notebooks/models/fine-tuned-gliner\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# predict evaluation set with the fine-tuned model\n",
    "\n",
    "# fine-tuned model is the new GLiNER\n",
    "fine_tuned_gliner = 'models/fine-tuned-gliner'\n",
    "\n",
    "# use the same labels you have defined for this lab\n",
    "labels = ['address', 'organization', 'person'] \n",
    "\n",
    "# Configuration for GLiNER integration\n",
    "custom_spacy_config = {\n",
    "    \"gliner_model\": fine_tuned_gliner,\n",
    "    \"chunk_size\": 250,\n",
    "    \"labels\": labels,\n",
    "    \"style\": \"ent\"\n",
    "}\n",
    "\n",
    "# Initialize a blank English spaCy pipeline and add GLiNER\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"gliner_spacy\", config=custom_spacy_config)\n",
    "\n",
    "text = \"clothes clothing dress tshirt\"\n",
    "\n",
    "# Process the text with the pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Output detected entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = convert_data_to_nervaluate_format(eval_data)\n",
    "fine_tuned_preds = convert_data_to_nervaluate_format(fine_tuned_gliner_preds)\n",
    "baseline_preds = convert_data_to_nervaluate_format(baseline_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_baseline = Evaluator(true, baseline_preds, tags=labels)\n",
    "baseline_results, baseline_results_per_tag, baseline_result_indices, baseline_result_indices_by_tag = evaluator_fine_tuned.evaluate()\n",
    "\n",
    "print(f\"Precision: {baseline_results['ent_type']['precision']}\\nRecall: {baseline_results['ent_type']['recall']}\\nF1: {baseline_results['ent_type']['f1']}\")\n",
    "\n",
    "evaluator_fine_tuned = Evaluator(true, fine_tuned_preds, tags=labels)\n",
    "fine_tuned_results, fine_tuned_results_per_tag, fine_tuned_result_indices, fine_tuned_result_indices_by_tag = evaluator_fine_tuned.evaluate()\n",
    "\n",
    "print(f\"Precision: {fine_tuned_results['ent_type']['precision']}\\nRecall: {fine_tuned_results['ent_type']['recall']}\\nF1: {fine_tuned_results['ent_type']['f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the precision, recall, and f1 score for each entity type\n",
    "# it's useful to understand how the model performs on each entity type\n",
    "def get_per_entity_type_results(results_per_tag, labels):\n",
    "    scores_per_entity_type = []\n",
    "    for entity_type in labels:\n",
    "        entity_type_scores = {'entity_type': entity_type,\n",
    "                              'precision': results_per_tag[entity_type]['partial']['precision'],\n",
    "                              'recall': results_per_tag[entity_type]['partial']['recall'],\n",
    "                              'f1': results_per_tag[entity_type]['partial']['f1']\n",
    "                              }\n",
    "        scores_per_entity_type.append(entity_type_scores)\n",
    "\n",
    "    return scores_per_entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_results_per_tag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m baseline_per_entity_type_scores \u001b[38;5;241m=\u001b[39m get_per_entity_type_results(\u001b[43mbaseline_results_per_tag\u001b[49m, labels)\n\u001b[1;32m      3\u001b[0m fine_tuned_entity_type_scores \u001b[38;5;241m=\u001b[39m get_per_entity_type_results(fine_tuned_results_per_tag, labels)\n\u001b[1;32m      4\u001b[0m baseline_scores \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataframe\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline_results_per_tag' is not defined"
     ]
    }
   ],
   "source": [
    "baseline_per_entity_type_scores = get_per_entity_type_results(baseline_results_per_tag, labels)\n",
    "fine_tuned_entity_type_scores = get_per_entity_type_results(fine_tuned_results_per_tag, labels)\n",
    "baseline_scores = pd.DataFrame(baseline_per_entity_type_scores)\n",
    "fine_tuned_scores = pd.DataFrame(fine_tuned_entity_type_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_per_entity_type_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_entity_type_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero-shot-ner-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
