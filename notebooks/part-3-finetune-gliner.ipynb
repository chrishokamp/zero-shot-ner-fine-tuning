{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook copied from here:\n",
    "# https://github.com/urchade/GLiNER/blob/main/examples/finetune.ipynb\n",
    "\n",
    "import json\n",
    "from gliner import GLiNER\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import os\n",
    "\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NER Dataset for Fine-Tuning\n",
    "\n",
    "**TODO**: you need to load your own NER dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE DATASET\n",
    "# from https://github.com/urchade/GLiNER/blob/main/examples/sample_data.json\n",
    "# train_path = \"sample_data.json\" \n",
    "\n",
    "# with open(train_path, \"r\") as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading datasets generated in part-2\n",
    "train_data = read_jsonl('data/generated_dataset_train.jsonl')\n",
    "train_data = [x for x in train_data if len(x[\"ner\"]) > 0]\n",
    "eval_data = read_jsonl('data/generated_dataset_eval.jsonl')\n",
    "len(train_data), len(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenized_text': ['You',\n",
       "  'can',\n",
       "  'look',\n",
       "  'at',\n",
       "  'the',\n",
       "  'construction',\n",
       "  'after',\n",
       "  'you',\n",
       "  'have',\n",
       "  'analyzed',\n",
       "  'the',\n",
       "  'poem',\n",
       "  'and',\n",
       "  'have',\n",
       "  'better',\n",
       "  'luck',\n",
       "  'finding',\n",
       "  'out',\n",
       "  'why',\n",
       "  'the',\n",
       "  'devices',\n",
       "  'are',\n",
       "  'used',\n",
       "  'that',\n",
       "  'way',\n",
       "  '.'],\n",
       " 'ner': [[20, 21, 'tool']]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demiangholipour/miniconda3/envs/zero-shot-ner-lab/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# available models: https://huggingface.co/urchade\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "# Define the hyperparameters in a config variable\n",
    "config = SimpleNamespace(\n",
    "    # num_steps=10000, # number of training iteration\n",
    "    num_steps=10, # low number of steps for testing NOTE: MODIFY THIS\n",
    "    train_batch_size=2, \n",
    "    eval_every=1000, # evaluation/saving steps\n",
    "    save_directory=\"logs\", # where to save checkpoints\n",
    "    warmup_ratio=0.1, # warmup steps\n",
    "    device='cpu',\n",
    "    lr_encoder=1e-5, # learning rate for the backbone\n",
    "    lr_others=5e-5, # learning rate for other parameters\n",
    "    freeze_token_rep=False, # freeze of not the backbone\n",
    "    \n",
    "    # Parameters for set_sampling_params\n",
    "    max_types=25, # maximum number of entity types during training\n",
    "    shuffle_types=True, # if shuffle or not entity types\n",
    "    random_drop=True, # randomly drop entity types\n",
    "    max_neg_type_ratio=1, # ratio of positive/negative types, 1 mean 50%/50%, 2 mean 33%/66%, 3 mean 25%/75% ...\n",
    "    max_len=384 # maximum sentence length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, config, train_data, eval_data=None):\n",
    "    model = model.to(config.device)\n",
    "\n",
    "    # Set sampling parameters from config\n",
    "    model.set_sampling_params(\n",
    "        max_types=config.max_types, \n",
    "        shuffle_types=config.shuffle_types, \n",
    "        random_drop=config.random_drop, \n",
    "        max_neg_type_ratio=config.max_neg_type_ratio, \n",
    "        max_len=config.max_len\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # Initialize data loaders\n",
    "    train_loader = model.create_dataloader(train_data, batch_size=config.train_batch_size, shuffle=True)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = model.get_optimizer(config.lr_encoder, config.lr_others, config.freeze_token_rep)\n",
    "\n",
    "    pbar = tqdm(range(config.num_steps))\n",
    "\n",
    "    if config.warmup_ratio < 1:\n",
    "        num_warmup_steps = int(config.num_steps * config.warmup_ratio)\n",
    "    else:\n",
    "        num_warmup_steps = int(config.warmup_ratio)\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=config.num_steps\n",
    "    )\n",
    "\n",
    "    iter_train_loader = iter(train_loader)\n",
    "\n",
    "    for step in pbar:\n",
    "        try:\n",
    "            x = next(iter_train_loader)\n",
    "        except StopIteration:\n",
    "            iter_train_loader = iter(train_loader)\n",
    "            x = next(iter_train_loader)\n",
    "\n",
    "        for k, v in x.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                x[k] = v.to(config.device)\n",
    "\n",
    "        loss = model(x)  # Forward pass\n",
    "            \n",
    "        # Check if loss is nan\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        description = f\"step: {step} | epoch: {step // len(train_loader)} | loss: {loss.item():.2f}\"\n",
    "        pbar.set_description(description)\n",
    "\n",
    "        if (step + 1) % config.eval_every == 0:\n",
    "\n",
    "            model.eval()\n",
    "            \n",
    "            if eval_data is not None:\n",
    "                results, f1 = model.evaluate(eval_data[\"samples\"], flat_ner=True, threshold=0.5, batch_size=12,\n",
    "                                     entity_types=eval_data[\"entity_types\"])\n",
    "\n",
    "                print(f\"Step={step}\\n{results}\")\n",
    "\n",
    "            if not os.path.exists(config.save_directory):\n",
    "                os.makedirs(config.save_directory)\n",
    "                \n",
    "            model.save_pretrained(f\"{config.save_directory}/finetuned_{step}\")\n",
    "\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step: 9 | epoch: 0 | loss: 12.24: 100%|██████████| 10/10 [00:10<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# You can also define evaluation data manually here.\n",
    "# For now, evaluation only support fix entity types (but can be easily extended)\n",
    "# eval_data = {\n",
    "#     \"entity_types\": [\"Person\", 'Event Reservation'],\n",
    "#     \"samples\": data[:10]\n",
    "# }\n",
    "\n",
    "train(model, config, train_data, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"custom-model-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in /home/demiangholipour/projects/zero-shot-ner-fine-tuning/notebooks/custom-model-small\n"
     ]
    }
   ],
   "source": [
    "loaded_model = GLiNER.from_pretrained(\"custom-model-small\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenized_text': ['Ampro',\n",
       "  'Little',\n",
       "  'Board',\n",
       "  '(',\n",
       "  'TM',\n",
       "  ')',\n",
       "  'Complete',\n",
       "  'CNC',\n",
       "  'assembly',\n",
       "  '.',\n",
       "  'Ampro',\n",
       "  'Little',\n",
       "  'Board',\n",
       "  '(',\n",
       "  'TM',\n",
       "  ')',\n",
       "  'Complete',\n",
       "  'CNC',\n",
       "  'assembly',\n",
       "  'Repaired',\n",
       "  'and',\n",
       "  'Working',\n",
       "  '.',\n",
       "  'If',\n",
       "  'you',\n",
       "  'have',\n",
       "  'any',\n",
       "  'technical',\n",
       "  'questions',\n",
       "  'or',\n",
       "  'wish',\n",
       "  'to',\n",
       "  'offer',\n",
       "  'technical',\n",
       "  'information',\n",
       "  'about',\n",
       "  'any',\n",
       "  'Ampro',\n",
       "  'units',\n",
       "  'then',\n",
       "  'fill',\n",
       "  'out',\n",
       "  'the',\n",
       "  'form',\n",
       "  'below',\n",
       "  '.'],\n",
       " 'ner': [[7, 8, 'tool'], [17, 18, 'tool']]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tool']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_types = sorted(set([t for item in eval_data for _, _, t in item['ner']]))\n",
    "entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict_entities(\"I'm using a screws to build my Ikea chair.\", labels=['tool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.00%\tR: 0.00%\tF1: 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demiangholipour/miniconda3/envs/zero-shot-ner-lab/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_str, f1 = loaded_model.evaluate(test_data=eval_data, flat_ner=True, threshold=0.5, batch_size=1, entity_types=entity_types)\n",
    "print(eval_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Nervaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_predictions = []\n",
    "for item in eval_data:\n",
    "    text = ' '.join(item[\"tokenized_text\"])\n",
    "    predictions = loaded_model.predict_entities(text, labels=entity_types)\n",
    "    eval_predictions.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_nervaluate(data):\n",
    "    \"\"\"\n",
    "    Example for required format:\n",
    "    true = [\n",
    "    [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n",
    "    [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n",
    "     {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n",
    "    ]\n",
    "    pred = [\n",
    "        [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n",
    "        [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n",
    "        {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        formatted_entities = []\n",
    "        for start, end, label in item['ner']:\n",
    "            formatted_entities.append({\"label\": label, \"start\": start, \"end\": end})\n",
    "        formatted_data.append(formatted_entities)\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_eval_data = format_data_for_nervaluate(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ent_type': {'actual': 0,\n",
      "              'correct': 0,\n",
      "              'f1': 0,\n",
      "              'incorrect': 0,\n",
      "              'missed': 9,\n",
      "              'partial': 0,\n",
      "              'possible': 9,\n",
      "              'precision': 0,\n",
      "              'recall': 0.0,\n",
      "              'spurious': 0},\n",
      " 'exact': {'actual': 0,\n",
      "           'correct': 0,\n",
      "           'f1': 0,\n",
      "           'incorrect': 0,\n",
      "           'missed': 9,\n",
      "           'partial': 0,\n",
      "           'possible': 9,\n",
      "           'precision': 0,\n",
      "           'recall': 0.0,\n",
      "           'spurious': 0},\n",
      " 'partial': {'actual': 0,\n",
      "             'correct': 0,\n",
      "             'f1': 0,\n",
      "             'incorrect': 0,\n",
      "             'missed': 9,\n",
      "             'partial': 0,\n",
      "             'possible': 9,\n",
      "             'precision': 0,\n",
      "             'recall': 0.0,\n",
      "             'spurious': 0},\n",
      " 'strict': {'actual': 0,\n",
      "            'correct': 0,\n",
      "            'f1': 0,\n",
      "            'incorrect': 0,\n",
      "            'missed': 9,\n",
      "            'partial': 0,\n",
      "            'possible': 9,\n",
      "            'precision': 0,\n",
      "            'recall': 0.0,\n",
      "            'spurious': 0}}\n"
     ]
    }
   ],
   "source": [
    "from nervaluate import Evaluator\n",
    "from pprint import pprint\n",
    "\n",
    "# true = [\n",
    "#     [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n",
    "#     [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n",
    "#      {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n",
    "# ]\n",
    "\n",
    "# pred = [\n",
    "#     [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n",
    "#     [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n",
    "#      {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n",
    "# ]\n",
    "\n",
    "# evaluator = Evaluator(true, pred, tags=['LOC', 'PER'])\n",
    "\n",
    "evaluator = Evaluator(formatted_eval_data, eval_predictions, tags=['tool'])\n",
    "\n",
    "# Returns overall metrics and metrics for each tag\n",
    "\n",
    "results, results_per_tag = evaluator.evaluate()\n",
    "\n",
    "pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
