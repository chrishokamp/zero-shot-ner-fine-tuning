{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook copied from here:\n",
    "# https://github.com/urchade/GLiNER/blob/main/examples/finetune.ipynb\n",
    "\n",
    "import json\n",
    "from gliner import GLiNER\n",
    "import spacy\n",
    "from gliner_spacy.pipeline import GlinerSpacy\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function loads jsonl data\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NER Dataset for Fine-Tuning\n",
    "\n",
    "**TODO**: you need to load your own NER dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(969, 31, 31)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = 'data/argilla_dataset_train.jsonl'\n",
    "eval_path = 'data/argilla_dataset_eval.jsonl'\n",
    "baseline_predictions_path = 'data/argilla_dataset_gliner_eval_preds.jsonl'\n",
    "\n",
    "train_data = read_jsonl(train_path)\n",
    "eval_data = read_jsonl(eval_path)\n",
    "baseline_predictions = read_jsonl(baseline_predictions_path)\n",
    "len(train_data), len(eval_data), len(baseline_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenized_text': ['Dress', ',', 'Shoes', ',', 'and', 'Scarf', 'provided', 'by', 'ModCloth', '.'], 'ner': [[0, 5, 'clothing'], [7, 12, 'clothing'], [18, 23, 'clothing'], [36, 44, 'organization']]}\n",
      "\n",
      "{'tokenized_text': ['You', 'can', 'bring', 'some', 'of', 'varying', 'weight', 'to', 'give', 'yourself', 'options', '.', 'Gaiters', 'â€“', 'these', 'will', 'protect', 'you', 'and', 'your', 'boots', 'from', 'rain', 'or', 'muck', '.'], 'ner': [[105, 110, 'clothing'], [63, 70, 'organization']]}\n"
     ]
    }
   ],
   "source": [
    "# checking for gliner format\n",
    "# {'tokenized_text' [], 'ner': [ [start_token_i, end_token_i, label], ...], ...}\n",
    "print(f\"{train_data[0]}\\n\\n{eval_data[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1163d0c3ce24eb5904153c1542ef820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egemenipek/miniconda3/envs/zero-shot-ner-lab/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# available models: https://huggingface.co/urchade\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_largev2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "from gliner import GLiNERConfig, GLiNER\n",
    "from gliner.training import Trainer, TrainingArguments\n",
    "from gliner.data_processing.collator import DataCollatorWithPadding, DataCollator\n",
    "from gliner.utils import load_config_as_namespace\n",
    "from gliner.data_processing import WordsSplitter, GLiNERDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a27b1c64e434b8d87a099b747aa254a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: mps:0\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # if you have apple m-series \n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # if you have gpu\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # you most likely have this :)\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_largev2\")\n",
    "\n",
    "# dynamic padding used to speed training up and save memory\n",
    "data_collator = DataCollator(model.config, data_processor=model.data_processor, prepare_labels=True)\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model is on: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epochs: 2, Number of Batches: 60, Number of Steps: 120\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2  # Define how many times you want to pass over the dataset\n",
    "batch_size = 16\n",
    "data_size = len(train_data)\n",
    "num_batches = data_size // batch_size  # Total batches per epoch\n",
    "num_steps = num_epochs * num_batches  # Total training steps\n",
    "print(f\"Number of Epochs: {num_epochs}, Number of Batches: {num_batches}, Number of Steps: {num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egemenipek/miniconda3/envs/zero-shot-ner-lab/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    others_lr=1e-5,\n",
    "    others_weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\", #cosine\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    focal_loss_alpha=0.75,\n",
    "    focal_loss_gamma=2,\n",
    "    num_train_epochs=num_epochs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    #save_steps = 100,\n",
    "    #save_total_limit=10,\n",
    "    dataloader_num_workers = 0,\n",
    "    use_cpu = False,\n",
    "    #report_to=\"none\",\n",
    "    )\n",
    "\n",
    "# this is to track loss during training\n",
    "class LossTrackerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []  # Store loss per step\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "            print(f\"Step {state.global_step}: Loss {logs['loss']}\")\n",
    "\n",
    "loss_tracker = LossTrackerCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c_/w5by8m2s30jcjd665yxczwx40000gp/T/ipykernel_72830/3698432713.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='122' max='122' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [122/122 02:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=122, training_loss=9.800071841380635, metrics={'train_runtime': 153.0584, 'train_samples_per_second': 12.662, 'train_steps_per_second': 0.797, 'total_flos': 0.0, 'train_loss': 9.800071841380635, 'epoch': 2.0})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    tokenizer=model.data_processor.transformer_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[loss_tracker]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/fine-tuned-gliner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in /Users/egemenipek/zero-shot-ner-fine-tuning-lab/notebooks/models/fine-tuned-gliner\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_gliner = GLiNER.from_pretrained(\"models/fine-tuned-gliner\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "text = 'dress is a clotthing item. Look at that jacket yo!'\n",
    "\n",
    "labels = ['clothing', 'organization', 'address', 'event']\n",
    "\n",
    "entities = fine_tuned_gliner.predict_entities(text, labels)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_types = sorted(set([t for item in eval_data for _, _, t in item['ner']]))\n",
    "entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.predict_entities(\"The Federal Reserve announced new guidelines for banking operations to enhance financial stability.\", labels=['regulator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_str, f1 = loaded_model.evaluate(test_data=eval_data, flat_ner=True, threshold=0.5, batch_size=1, entity_types=entity_types)\n",
    "print(eval_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Evaluate and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nervaluate import Evaluator\n",
    "\n",
    "# below function converts the data to the format that nervaluate expects\n",
    "def convert_data_to_nervaluate_format(data):\n",
    "    formatted_data = []\n",
    "    for data_point in data:\n",
    "        formatted_data_point = [{'label': ner_point[2], 'start': ner_point[0], 'end': ner_point[1]} for ner_point in data_point['ner']]\n",
    "        formatted_data.append(formatted_data_point)\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in /Users/egemenipek/zero-shot-ner-fine-tuning-lab/notebooks/models/fine-tuned-gliner\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# predict evaluation set with the fine-tuned model\n",
    "\n",
    "# fine-tuned model is the new GLiNER\n",
    "fine_tuned_gliner = 'models/fine-tuned-gliner'\n",
    "\n",
    "# use the same labels you have defined for this lab\n",
    "zero_shot_labels = ['address', 'organization', 'person'] \n",
    "\n",
    "# Configuration for GLiNER integration\n",
    "custom_spacy_config = {\n",
    "    \"gliner_model\": fine_tuned_gliner,\n",
    "    \"chunk_size\": 250,\n",
    "    \"labels\": zero_shot_labels,\n",
    "    \"style\": \"ent\"\n",
    "}\n",
    "\n",
    "# Initialize a blank English spaCy pipeline and add GLiNER\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"gliner_spacy\", config=custom_spacy_config)\n",
    "\n",
    "text = \"clothes clothing dress tshirt\"\n",
    "\n",
    "# Process the text with the pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Output detected entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "address address organization organization\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = convert_data_to_nervaluate_format(eval_data)\n",
    "pred = convert_data_to_nervaluate_format()\n",
    "baseline_pred = convert_data_to_nervaluate_format(baseline_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = convert_data_to_nervaluate_format(eval_set) # human annotated data in the format that nervaluate expects\n",
    "pred = convert_data_to_nervaluate_format(gliner_eval_preds) # GLiNER predictions in the format that nervaluate expects\n",
    "\n",
    "evaluator = Evaluator(true, pred, tags=labels)\n",
    "results, results_per_tag, result_indices, result_indices_by_tag = evaluator.evaluate()\n",
    "\n",
    "print(f\"Precision: {results['ent_type']['precision']}\\nRecall: {results['ent_type']['recall']}\\nF1: {results['ent_type']['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Nervaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_predictions = []\n",
    "for item in eval_data:\n",
    "    text = ' '.join(item[\"tokenized_text\"])\n",
    "    predictions = loaded_model.predict_entities(text, labels=entity_types)\n",
    "    eval_predictions.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_nervaluate(data):\n",
    "    \"\"\"\n",
    "    Example for required format:\n",
    "    true = [\n",
    "    [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n",
    "    [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n",
    "     {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n",
    "    ]\n",
    "    pred = [\n",
    "        [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n",
    "        [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n",
    "        {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        formatted_entities = []\n",
    "        for start, end, label in item['ner']:\n",
    "            formatted_entities.append({\"label\": label, \"start\": start, \"end\": end})\n",
    "        formatted_data.append(formatted_entities)\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_eval_data = format_data_for_nervaluate(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nervaluate import Evaluator\n",
    "from pprint import pprint\n",
    "\n",
    "# true = [\n",
    "#     [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n",
    "#     [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n",
    "#      {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n",
    "# ]\n",
    "\n",
    "# pred = [\n",
    "#     [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n",
    "#     [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n",
    "#      {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n",
    "# ]\n",
    "\n",
    "# evaluator = Evaluator(true, pred, tags=['LOC', 'PER'])\n",
    "\n",
    "evaluator = Evaluator(formatted_eval_data, eval_predictions, tags=['tool'])\n",
    "\n",
    "# Returns overall metrics and metrics for each tag\n",
    "\n",
    "results, results_per_tag = evaluator.evaluate()\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tokenized_text': ['I', 'was', 'searching', 'for', 'a', 'Property', 'and', 'found', 'this', 'listing', '(', 'MLS', 'Â®', '#', '10100074', ')', '.', 'Please', 'send', 'me', 'more', 'information', 'regarding', '16440', 'Cr', '178', '#', '1001', ',', 'Tyler', ',', 'TX', ',', '75703', '.'], 'ner': [[114, 150, 'address']]}, {'tokenized_text': ['Family', 'flat', 'located', 'on', 'an', 'upper', 'floor', '(', 'with', 'lift', ')', 'in', 'the', 'North', 'historical', 'Marais', 'area', '-', 'right', 'bank', 'of', 'Paris', '.'], 'ner': []}, {'tokenized_text': ['|a', 'Kerkham', ',', 'Roger', '.', '|0', 'http://id', '.', 'loc', '.', 'gov', '/', 'authorities', '/', 'names', '/', 'n50044780', '.'], 'ner': [[3, 17, 'person']]}, {'tokenized_text': ['.', '.', 'nice', 'and', 'airy', 'condo', '.', '.', '.', 'with', 'lovely', 'home', '-', 'cooked', 'dishes', 'by', 'Sugen', \"'s\", 'grandma', '!', 'All', 'from', 'scratch', '!', 'Such', 'a', 'talented', 'lady', 'who', 'can', 'also', 'crochet', 'and', 'cross', '-', 'stitch', '!'], 'ner': [[63, 68, 'person']]}, {'tokenized_text': ['The', 'members', 'of', 'staff', 'are', 'very', 'friendly', '.', 'Besides', 'the', '112', 'rooms', 'and', 'suites', ',', 'a', 'villa', 'containing', '5', 'bedrooms', 'can', 'be', 'booked', 'as', 'well', '.'], 'ner': []}, {'tokenized_text': ['in', 'Laurel', '.', '301', '-', '490', '-', '9200', '.', 'Movie', '--', 'The', 'Savage', 'library', 'branch', ',', '9525', 'Durness', 'Lane', ',', 'will', 'show', 'the', 'movie', 'Raiders', 'of', 'the', 'Lost', 'Ark', 'at', '7', 'p.', 'm.', 'Wednesday', '.', 'Registration', 'is', 'required', '.'], 'ner': [[61, 78, 'address']]}, {'tokenized_text': ['The', 'plant', 'represents', 'a', '$', '10', 'million', 'investment', 'by', 'Natron', 'Wood', 'Products', ',', 'Gov.'], 'ner': [[49, 69, 'organization']]}, {'tokenized_text': ['Angela', 'spent', 'her', 'childhood', 'at', 'Grezze', 'in', 'a', 'farm', '-', 'house', 'a', 'few', 'kilometers', 'away', 'from', 'the', 'city', 'if', 'Desezano', '.'], 'ner': [[0, 6, 'person']]}, {'tokenized_text': ['The', 'cycle', 'lane', 'up', 'West', 'Thomson', 'Street', 'ends', 'in', 'a', 'car', 'park', 'at', 'the', 'Clydebank', 'Health', 'Centre', '.', 'The', 'cycle', 'lane', 'up', 'West', 'Thomson', 'Street', '.', 'The', 'end', 'of', 'the', 'widened', 'footway', 'and', 'transition', 'onto', 'an', 'on', '-', 'road', 'painted', 'cycle', 'lane', 'along', 'Alderman', 'Road', '.'], 'ner': []}, {'tokenized_text': ['Nearly', 'a', 'third', 'of', 'properties', 'in', 'the', 'private', 'rented', 'sector', 'contain', 'major', 'safety', 'hazards', ',', 'according', 'to', 'data', 'contained', 'in', 'a', 'government', 'survey', 'of', 'UK', 'housing', 'stock', '.', 'The', 'annual', 'English', 'Housing', 'Survey', 'shows', 'that', '28', '.'], 'ner': []}, {'tokenized_text': ['Shingle', 'Express', ',', 'Inc.', 'has', 'been', 'providing', 'reliable', 'and', 'quality', 'gutter', 'installations', 'in', 'Thornton', ',', 'PA', 'since', '2009', '.'], 'ner': [[0, 21, 'organization']]}, {'tokenized_text': ['Nativity', 'Scene', \"'s\", 'Set', '.', 'Religious', 'Statue', \"'s\", ',', 'Holiday', 'Projector', \"'s\", '.', 'Authorized', 'for', 'General', 'Foam', 'Plastics', 'Corp', ',', 'Union', 'Products', 'Inc', ',', 'Mr', 'Christmas', ',', 'Empire', 'of', 'Carolina', '.'], 'ner': [[78, 104, 'organization'], [106, 124, 'organization'], [126, 138, 'person'], [140, 158, 'person']]}, {'tokenized_text': ['Simply', 'lift', 'the', 'front', 'sliding', 'door', 'to', 'access', 'the', 'pre', '-', 'drilled', 'areas', '.', 'â€¢', 'Your', 'modern', 'suggestion', 'box', '/', 'comment', 'box', 'ships', 'via', 'USPS', 'in', '7', '-', '10', 'business', 'days', 'following', 'receipt', 'of', 'payment', '.'], 'ner': [[121, 125, 'organization']]}, {'tokenized_text': ['This', 'turn', '-', 'key', 'restaurant', 'is', 'a', 'steal', 'at', '$', '130,000', ',', 'considering', 'the', 'inventory', 'is', 'included', ',', 'the', 'rent', 'is', '$', '1500', 'a', 'month', ',', 'and', 'the', 'owner', 'is', 'willing', 'to', 'discuss', 'all', 'aspects', 'of', 'the', 'restaurant', 'to', 'the', 'right', 'buyer', '.'], 'ner': []}, {'tokenized_text': ['The', 'old', 'church', 'by', 'the', 'lake', 'is', 'very', 'beautiful', '.', 'I', 'can', 'not', 'believe', 'that', 'she', 'did', 'that', 'to', 'me', '.', 'The', 'auditorium', 'is', 'terribly', 'stained', '.', 'It', 'is', ',', 'in', 'part', ',', 'due', 'to', 'acid', 'rain', '.'], 'ner': []}, {'tokenized_text': ['Submitted', 'by', 'Keby', 'on', 'We', 'd', ',', '05/21/2008', '-', '11:10', '.', 'I', 'went', 'to', 'my', 'local', 'newspaper', 'office', 'and', 'asked', 'if', 'they', 'have', 'any', 'old', 'newspapers', 'they', 'would', 'be', 'willing', 'to', 'give', 'me', '.'], 'ner': []}, {'tokenized_text': ['Patel', 'J.', 'B.', 'R.', 'Arts', ',', 'Patel', 'A.', 'M.', 'R.', 'Commerce', '&', 'Patel', 'J.', 'D.', 'K.', 'Davolwala', 'Sci', '.', 'College', ',', 'Borsad', 'is', 'located', 'at', 'Borsad-388540', ',', 'Dist', ':', 'Anand', ',', 'Gujarat', '.'], 'ner': [[0, 14, 'person'], [21, 44, 'person'], [47, 84, 'organization'], [107, 120, 'address']]}, {'tokenized_text': ['wrought', 'iron', 'baby', 'cribs', 'safe', 'restoration', 'hardware', 'used', 'crib', 'tufted', 'headboard', 'he', '.'], 'ner': []}, {'tokenized_text': ['We', 'offer', 'Hypnotherapy', 'services', 'for', 'Kesgrave', ',', 'England', '.', 'You', 'can', 'see', 'who', 'we', 'have', 'provided', 'Hypnotherapy', 'services', 'for', 'around', 'Kesgrave', ',', 'England', 'below', '.', 'Please', 'contact', 'us', 'if', 'you', 'have', 'any', 'questions', '.'], 'ner': []}, {'tokenized_text': ['The', 'retro', 'futuristic', 'stations', 'have', 'an', 'elevator', 'up', 'to', 'the', 'top', 'level', 'which', 'is', 'where', 'the', 'cable', 'car', 'comes', 'and', 'leaves', 'from', '.', 'Kenmore', 'station', 'is', 'the', 'hub', 'of', 'the', 'entire', 'system', '.'], 'ner': []}]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero-shot-ner-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
