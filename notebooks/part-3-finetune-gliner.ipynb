{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook copied from here:\n",
    "# https://github.com/urchade/GLiNER/blob/main/examples/finetune.ipynb\n",
    "\n",
    "import json\n",
    "from gliner import GLiNER\n",
    "import spacy\n",
    "from gliner_spacy.pipeline import GlinerSpacy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import os\n",
    "import pandas as pd\n",
    "from nervaluate import Evaluator\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "from gliner import GLiNERConfig, GLiNER\n",
    "from gliner.training import Trainer, TrainingArguments\n",
    "from gliner.data_processing.collator import DataCollatorWithPadding, DataCollator\n",
    "from gliner.utils import load_config_as_namespace\n",
    "from gliner.data_processing import WordsSplitter, GLiNERDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NER Dataset for Fine-Tuning\n",
    "\n",
    "**TODO**: you need to load your own NER dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function loads jsonl data\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/argilla_dataset_train.jsonl'\n",
    "train_data_str_path = 'data/argilla_dataset_train_set_str.jsonl'\n",
    "eval_path = 'data/argilla_dataset_eval.jsonl'\n",
    "eval_data_str_path = 'data/argilla_dataset_eval_set_str.jsonl'\n",
    "baseline_predictions_path = 'data/argilla_dataset_baseline_preds.jsonl'\n",
    "\n",
    "train_data = read_jsonl(train_path)\n",
    "train_data_str = read_jsonl(train_data_str_path)\n",
    "eval_data = read_jsonl(eval_path)\n",
    "eval_data_str = read_jsonl(eval_data_str_path)\n",
    "baseline_predictions = read_jsonl(baseline_predictions_path)\n",
    "\n",
    "len(train_data), len(train_data_str), len(eval_data), len(eval_data_str), len(baseline_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for gliner format\n",
    "# {'tokenized_text' [], 'ner': [ [start_token_i, end_token_i, label], ...], ...}\n",
    "print(f\"{train_data[0]}\\n\\n{eval_data[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-Trained GLiNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available models: https://huggingface.co/urchade\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_largev2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # if you have apple m-series \n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # if you have gpu\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # you most likely have this :)\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_largev2\")\n",
    "\n",
    "# dynamic padding used to speed training up and save memory\n",
    "data_collator = DataCollator(model.config, data_processor=model.data_processor, prepare_labels=True)\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model is on: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1  # Define how many times you want to pass over the dataset\n",
    "batch_size = 16\n",
    "data_size = len(train_data)\n",
    "num_batches = data_size // batch_size  # Total batches per epoch\n",
    "num_steps = num_epochs * num_batches  # Total training steps\n",
    "print(f\"Number of Epochs: {num_epochs}, Number of Batches: {num_batches}, Number of Steps: {num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    others_lr=1e-5,\n",
    "    others_weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\", #cosine\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    focal_loss_alpha=0.75,\n",
    "    focal_loss_gamma=2,\n",
    "    num_train_epochs=num_epochs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    #save_steps = 100,\n",
    "    #save_total_limit=10,\n",
    "    dataloader_num_workers = 0,\n",
    "    use_cpu = False,\n",
    "    #report_to=\"none\",\n",
    "    )\n",
    "\n",
    "# this is to track loss during training\n",
    "class LossTrackerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []  # Store loss per step\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "            print(f\"Step {state.global_step}: Loss {logs['loss']}\")\n",
    "\n",
    "loss_tracker = LossTrackerCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    tokenizer=model.data_processor.transformer_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[loss_tracker]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/fine-tuned-gliner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possibly, remove from this cell onward from this section\n",
    "fine_tuned_gliner = GLiNER.from_pretrained(\"models/fine-tuned-gliner\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'dress is a clotthing item. Look at that jacket yo!'\n",
    "\n",
    "labels = ['clothing', 'organization', 'address', 'event']\n",
    "\n",
    "entities = fine_tuned_gliner.predict_entities(text, labels)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in train_data_str:\n",
    "    entities = fine_tuned_gliner.predict_entities(example, labels)\n",
    "    if entities:\n",
    "        print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict, Evaluate and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the fine-tuned model first\n",
    "\n",
    "# fine-tuned model is the new GLiNER\n",
    "fine_tuned_gliner = 'models/fine-tuned-gliner'\n",
    "\n",
    "# use the same labels you have defined for this lab\n",
    "labels = ['clothing', 'organization', 'address', 'event']\n",
    "\n",
    "# config for fine-tuned gliner's spacy integration\n",
    "custom_spacy_config = {\n",
    "    \"gliner_model\": fine_tuned_gliner,\n",
    "    \"chunk_size\": 250,\n",
    "    \"labels\": labels,\n",
    "    \"style\": \"ent\"\n",
    "}\n",
    "\n",
    "# Initialize a blank English spaCy pipeline and add fine-tuned GLiNER\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"gliner_spacy\", config=custom_spacy_config)\n",
    "\n",
    "text = \"clothes clothing dress tshirt\"\n",
    "\n",
    "# Process the text with the pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Output detected entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect predictions of the fine-tuned model on the evaluation data\n",
    "fine_tuned_preds = []\n",
    "for example in eval_data_str:\n",
    "    doc = nlp(example)\n",
    "    doc_ents_nervaluate_format = []\n",
    "    for ent in doc.ents:\n",
    "        ent_nervulate_format = {'label': ent.label_, 'start': ent.start_char, 'end': ent.end_char}\n",
    "        doc_ents_nervaluate_format.append(ent_nervulate_format)\n",
    "    fine_tuned_preds.append(doc_ents_nervaluate_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below function converts the data to the format that nervaluate expects\n",
    "def convert_data_to_nervaluate_format(data):\n",
    "    formatted_data = []\n",
    "    for data_point in data:\n",
    "        formatted_data_point = [{'label': ner_point[2], 'start': ner_point[0], 'end': ner_point[1]} for ner_point in data_point['ner']]\n",
    "        formatted_data.append(formatted_data_point)\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the loaded data into nervaluate format\n",
    "true = convert_data_to_nervaluate_format(eval_data)\n",
    "baseline_preds = convert_data_to_nervaluate_format(baseline_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the baseline and get overall scores\n",
    "baseline_evaluator = Evaluator(true, baseline_preds, tags=labels)\n",
    "baseline_results, baseline_results_per_tag, baseline_result_indices, baseline_result_indices_by_tag = baseline_evaluator.evaluate()\n",
    "\n",
    "print(f\"Precision: {baseline_results['ent_type']['precision']}\\nRecall: {baseline_results['ent_type']['recall']}\\nF1: {baseline_results['ent_type']['f1']}\")\n",
    "\n",
    "print('----')\n",
    "\n",
    "# evaluate the fine-tuned model and get overall scores\n",
    "fine_tuned_evaluator = Evaluator(true, fine_tuned_preds, tags=labels)\n",
    "fine_tuned_results, fine_tuned_results_per_tag, fine_tuned_result_indices, fine_tuned_result_indices_by_tag = fine_tuned_evaluator.evaluate()\n",
    "\n",
    "print(f\"Precision: {fine_tuned_results['ent_type']['precision']}\\nRecall: {fine_tuned_results['ent_type']['recall']}\\nF1: {fine_tuned_results['ent_type']['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the fine-tuned model's per-entity-type performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the precision, recall, and f1 score for each entity type\n",
    "# it's useful to understand how the model performs on each entity type\n",
    "def get_per_entity_type_results(results_per_tag, labels):\n",
    "    scores_per_entity_type = []\n",
    "    for entity_type in labels:\n",
    "        entity_type_scores = {'entity_type': entity_type,\n",
    "                              'precision': results_per_tag[entity_type]['partial']['precision'],\n",
    "                              'recall': results_per_tag[entity_type]['partial']['recall'],\n",
    "                              'f1': results_per_tag[entity_type]['partial']['f1']\n",
    "                              }\n",
    "        scores_per_entity_type.append(entity_type_scores)\n",
    "\n",
    "    return scores_per_entity_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_per_entity_type_scores = get_per_entity_type_results(baseline_results_per_tag, labels)\n",
    "fine_tuned_entity_type_scores = get_per_entity_type_results(fine_tuned_results_per_tag, labels)\n",
    "baseline_scores = pd.DataFrame(baseline_per_entity_type_scores)\n",
    "fine_tuned_scores = pd.DataFrame(fine_tuned_entity_type_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero-shot-ner-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
