{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82G2vlxphIlD"
   },
   "source": [
    "# üí´  Explore and analyze NER predictions\n",
    "\n",
    "In this tutorial, we will learn to log [spaCy](https://spacy.io/) Name Entity Recognition (NER) predictions.\n",
    "\n",
    "This is useful for:\n",
    "\n",
    "- üßêEvaluating pre-trained models.\n",
    "- üîéSpotting frequent errors both during development and production.\n",
    "- üìàAnnotating records to create an gold-standard evaluation dataset.\n",
    "\n",
    "\n",
    "Reference: https://docs.argilla.io/en/latest/tutorials/notebooks/labelling-tokenclassification-spacy-pretrained.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8phxyG-hIlF"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will learn how to explore and analyze spaCy NER pipelines in an easy way.\n",
    "\n",
    "We will load the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub and use a transformer-based spaCy model for detecting entities in this dataset and log the detected entities into an Argilla dataset. This dataset can be used for exploring the quality of predictions and for creating a new training set, by correcting, adding and validating entities via human annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, run your Argilla server if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the argilla docker container in your terminal (or use the %%bash magic)\n",
    "# This may take a couple of minutes to spin up the container\n",
    "# docker run -d --name quickstart -p 6900:6900 argilla/argilla-quickstart:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp0MOU97hIlG"
   },
   "source": [
    "Let's import the Argilla module for reading and writing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "gG2ZaK1OhIlG"
   },
   "outputs": [],
   "source": [
    "import argilla as rg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1rV-N6phIlG"
   },
   "source": [
    "If you are running Argilla using the Docker quickstart image or Hugging Face Spaces, you need to init the Argilla client with the `URL` and `API_KEY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "dT-F6e7zhIlG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"850\"\n",
       "            height=\"600\"\n",
       "            src=\"http://localhost:6900?frameborder=0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x171a4a020>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Argilla has been deployed at: http://localhost:6900"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace api_url with the url to your HF Spaces URL if using Spaces\n",
    "# Replace api_key if you configured a custom API key\n",
    "# default argilla username is 'argilla'\n",
    "# default argilla password is '12345678'\n",
    "# default api_key for argilla on docker is 'argilla.apikey'\n",
    "client = rg.Argilla(\n",
    "    api_url=\"http://localhost:6900\",                      # If you are using the docker container\n",
    "    # api_url=\"https://jackboyla-zero-shot-lab.hf.space\", # If you are using HF Spaces\n",
    "    api_key=\"argilla.apikey\"\n",
    ")\n",
    "client # Test your login! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEU7UgpthIlH"
   },
   "source": [
    "Finally, let's include the imports we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "GktYugZxhIlH"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPJ5kkX7hIlH"
   },
   "source": [
    "## Our dataset\n",
    "For this tutorial, our default dataset is the [*Gutenberg Time*](https://huggingface.co/datasets/gutenberg_time) dataset from the Hugging Face Hub. It contains all explicit time references in a dataset of 52,183 novels whose full text is available via Project Gutenberg. From extracts of novels, we are surely going to find some NER entities.\n",
    "\n",
    "If you are following the full lab, you can also load the dataset you generated in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "MEURV43lhIlH",
    "outputId": "c8ef60fe-55d6-441f-b65b-a2ce6e3801cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               input  \\\n",
      "0  When you would like the property photographed....   \n",
      "1  I want to go see this property in person! MLS#...   \n",
      "2  Va. 7 property for Preston Propane before the ...   \n",
      "3  I was searching for a Property and found this ...   \n",
      "4  I would like more information about 7362 Horiz...   \n",
      "\n",
      "                                              output  \\\n",
      "0  ['property <> real estate <> physical entity r...   \n",
      "1  ['MLS# 234382 <> Real Estate Property <> Uniqu...   \n",
      "2  ['Va. 7 <> Street Address <> Physical location...   \n",
      "3  ['Property <> Real estate <> Physical asset wi...   \n",
      "4  ['7362 Horizon Drive West Palm Beach, FL 33412...   \n",
      "\n",
      "                                          embeddings  \n",
      "0  [0.0165400430560112, 0.04142063856124878, -0.0...  \n",
      "1  [0.003052317537367344, 0.013402799144387245, -...  \n",
      "2  [0.014098312705755234, 0.024646742269396782, 0...  \n",
      "3  [0.009898650459945202, 0.0028502296190708876, ...  \n",
      "4  [0.011095905676484108, 0.03647858276963234, 0....  \n"
     ]
    }
   ],
   "source": [
    "# when using this notebook standalone, choose a dataset from the hub\n",
    "# dataset = load_dataset(\"gutenberg_time\", split=\"train\", streaming=True)\n",
    "\n",
    "# when using this notebook as part of the full lab, load the dataset you created in the previous step\n",
    "DATASET_PATH = 'data/sampled_dataset/'\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "\n",
    "# Let's have a look at the first 5 examples of the train set.\n",
    "try:\n",
    "    print(pd.DataFrame(dataset.take(5)))\n",
    "except AttributeError:\n",
    "    print(pd.DataFrame(dataset[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVmbkS2WhIlH"
   },
   "source": [
    "## Annotating with GLiNER and Logging NER entities into Argilla\n",
    "\n",
    "\n",
    "Let's instantiate a spaCy transformer `nlp` pipeline and apply it to the first N examples in our dataset, collecting the *tokens* and *NER entities*.\n",
    "\n",
    "We're going to use a [GLiNER](https://github.com/urchade/GLiNER) model to perform zero shot NER. This means we can provide any entity labels we like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0143288d50ed43fe9479fc4c95a478ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egemenipek/miniconda3/envs/zero-shot-ner-lab/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill Gates person\n",
      "Microsoft organization\n"
     ]
    }
   ],
   "source": [
    "# observe how gliner works :)\n",
    "\n",
    "import spacy\n",
    "from gliner_spacy.pipeline import GlinerSpacy\n",
    "\n",
    "# Gliner model options https://huggingface.co/urchade \n",
    "gliner_model = \"urchade/gliner_largev2\"\n",
    "\n",
    "# Define your domain here: the list of entity types you expect to see\n",
    "zero_shot_labels = [\"person\", \"organization\", \"email\", \"sports team\", \"business\"]\n",
    "\n",
    "# Configuration for GLiNER integration\n",
    "custom_spacy_config = {\n",
    "    \"gliner_model\": gliner_model,\n",
    "    \"chunk_size\": 250,\n",
    "    \"labels\": zero_shot_labels,\n",
    "    \"style\": \"ent\"\n",
    "}\n",
    "\n",
    "# Initialize a blank English spaCy pipeline and add GLiNER\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"gliner_spacy\", config=custom_spacy_config)\n",
    "\n",
    "# Example\n",
    "text = \"This is a text about Bill Gates and Microsoft.\"\n",
    "\n",
    "# Process the text with the pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Output detected entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create Argilla Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you restart kernel and want to continue from here, get the variable set again\n",
    "# because the created dataset will persist and you'll get an error trying to recreate the argilla_dataset variable from scratch\n",
    "for dataset in client.datasets.list():\n",
    "    if dataset.name == \"argilla_dataset\":\n",
    "        argilla_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you'd like to delete argilla_dataset and start over, just run this cell\n",
    "\n",
    "dataset_to_delete = client.datasets(name=\"argilla_dataset\")\n",
    "\n",
    "dataset_deleted = dataset_to_delete.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"gutenberg_spacy_ner\"\n",
    "dataset_name = \"argilla_dataset\"\n",
    "labels = ['address', 'organization', 'person'] # define a list of labels that you are interested in extracting with GLiNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5909663e010942c0a9533f6f7f6491c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egemenipek/miniconda3/envs/zero-shot-ner-lab/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gliner_spacy.pipeline.GlinerSpacy at 0x5bd0c5420>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration for GLiNER integration with the labels defined above\n",
    "custom_spacy_config = {\n",
    "    \"gliner_model\": gliner_model,\n",
    "    \"chunk_size\": 250,\n",
    "    \"labels\": labels,\n",
    "    \"style\": \"ent\"\n",
    "}\n",
    "\n",
    "# Initialize a blank English spaCy pipeline and add GLiNER\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"gliner_spacy\", config=custom_spacy_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create settings for Argilla\n",
    "settings = rg.Settings(\n",
    "    guidelines=\"Classify individual tokens into given labels\",\n",
    "    fields = [\n",
    "        rg.TextField(\n",
    "            name='text', # give it a name\n",
    "            title='Text', # this will be displayed on the UI above the text field\n",
    "            use_markdown=False # not necessary for this application\n",
    "        )\n",
    "    ],\n",
    "    # In Argilla a question is basically an annotation instance.\n",
    "    # This is a token classification case and Argilla has a built-in question type for that called the SpanQuestion.\n",
    "    questions=[\n",
    "        rg.SpanQuestion( \n",
    "            name=\"span_label\",\n",
    "            field='text',\n",
    "            labels=labels,\n",
    "            title=\"Classify individual tokens into given labels\",\n",
    "            allow_overlapping=False\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(id=UUID('a338067b-ce33-4c2f-accb-1f7374199351') inserted_at=datetime.datetime(2025, 3, 11, 21, 55, 3, 876417) updated_at=datetime.datetime(2025, 3, 11, 21, 55, 3, 909699) name='argilla_dataset' status='ready' guidelines='Classify individual tokens into given labels' allow_extra_metadata=False distribution=OverlapTaskDistributionModel(strategy='overlap', min_submitted=1) workspace_id=UUID('cf04c60d-319c-423e-b686-914e4f1a7ace') last_activity_at=datetime.datetime(2025, 3, 11, 21, 55, 3, 909699))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the dataset\n",
    "argilla_dataset = rg.Dataset(\n",
    "    name=dataset_name,\n",
    "    settings=settings\n",
    ")\n",
    "argilla_dataset.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Datasets</h3><table><tr><th>name</th><th>id</th><th>workspace_id</th><th>updated_at</th></tr><tr><td>argilla_dataset</td><td>a338067b-ce33-4c2f-accb-1f7374199351</td><td>cf04c60d-319c-423e-b686-914e4f1a7ace</td><td>2025-03-11T21:55:03.909699</td></tr></table>"
      ],
      "text/plain": [
       "<argilla.client.Datasets at 0x425076e90>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe that your dataset is created\n",
    "client.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending records...: 4batch [00:02,  1.55batch/s]                    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetRecords(Dataset(id=UUID('a338067b-ce33-4c2f-accb-1f7374199351') inserted_at=datetime.datetime(2025, 3, 11, 21, 55, 3, 876417) updated_at=datetime.datetime(2025, 3, 11, 21, 55, 3, 909699) name='argilla_dataset' status='ready' guidelines='Classify individual tokens into given labels' allow_extra_metadata=False distribution=OverlapTaskDistributionModel(strategy='overlap', min_submitted=1) workspace_id=UUID('cf04c60d-319c-423e-b686-914e4f1a7ace') last_activity_at=datetime.datetime(2025, 3, 11, 21, 55, 3, 909699)))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In argilla, you can log what they call 'suggestions' for each data instance\n",
    "# Suggestions are basically model predictions on the data instances\n",
    "\n",
    "# Having these suggestions has two main benefits:\n",
    "# 1. It can help you quickly label your data\n",
    "# 2. You can eyeball the performance of your baseline model\n",
    "\n",
    "# the below function will produce a suggestion output that is acceptable to Argilla from the GLiNER model\n",
    "def gliner_predict(text):\n",
    "    doc = nlp(text)\n",
    "    return [\n",
    "        {\"start\": ent.start_char, \"end\": ent.end_char, \"label\": ent.label_}\n",
    "        for ent in doc.ents\n",
    "    ]\n",
    "\n",
    "# collect all records here\n",
    "record_instances = []\n",
    "for row in dataset:\n",
    "    text = row['input'] # this is the input sequence\n",
    "    span_label= gliner_predict(text) # suggestion data\n",
    "    \n",
    "    # make the record instance acceptable by Argilla token classification task\n",
    "    record_instance = {\n",
    "        'text': text,\n",
    "        'span_label': span_label\n",
    "    }\n",
    "    record_instances.append(record_instance)\n",
    "\n",
    "argilla_dataset.records.log(record_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You can now view and annotate your dataset in Argilla.\n",
    "<p>For docker: http://localhost:6900/\n",
    "<p>or visit your space in HF spaces if you opted to use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'aa104c44-438d-46e2-bec0-db25b8951c6e', 'fields': {'text': 'If there is a criminal investigation, they can get a search warrant from a magistrate.'}, 'metadata': {}, 'suggestions': {'span_label': {'value': [], 'score': None, 'agent': None}}, 'responses': {'span_label': [{'value': [], 'user_id': 'ff683590-024e-4424-acef-de69dcd4f186'}]}, 'vectors': {}, 'status': 'completed', '_server_id': 'c1541566-065f-42b8-abaf-402b31f70d62'}\n",
      "{'span_label': [{'value': [], 'user_id': 'ff683590-024e-4424-acef-de69dcd4f186'}]}\n",
      "{'id': '1ca9f194-0b04-49ea-b051-f26f03e9befb', 'fields': {'text': 'Sightseeing on the way. Once you reach Thekkady, proceed to the cottage in teh resort.'}, 'metadata': {}, 'suggestions': {'span_label': {'value': [], 'score': None, 'agent': None}}, 'responses': {'span_label': [{'value': [], 'user_id': 'ff683590-024e-4424-acef-de69dcd4f186'}]}, 'vectors': {}, 'status': 'completed', '_server_id': '3852d152-64f5-43f4-86e5-02dbe9279c4c'}\n",
      "{'span_label': [{'value': [], 'user_id': 'ff683590-024e-4424-acef-de69dcd4f186'}]}\n",
      "{'id': 'e022c46b-76ca-4300-ab00-fa9504eebe22', 'fields': {'text': 'For Oliver Peoples, we designed, \"the Orb\", an object that serves 3 functions: appealing product display, eye-catching window signage, and representative brand icon.'}, 'metadata': {}, 'suggestions': {'span_label': {'value': [{'label': 'organization', 'start': 4, 'end': 18}], 'score': None, 'agent': None}}, 'responses': {'span_label': [{'value': [{'label': 'organization', 'start': 4, 'end': 18}], 'user_id': 'ff683590-024e-4424-acef-de69dcd4f186'}]}, 'vectors': {}, 'status': 'completed', '_server_id': '357c1c97-2b6f-4c76-94ea-c4760e359d83'}\n",
      "{'span_label': [{'value': [{'label': 'organization', 'start': 4, 'end': 18}], 'user_id': 'ff683590-024e-4424-acef-de69dcd4f186'}]}\n",
      "{'id': '41302f3d-b81a-492a-8e79-4a02a499c7ce', 'fields': {'text': 'J. and Isaiah. Visitation Sunday 1-7 P. M. Funeral Service Monday 11:00 A. M.'}, 'metadata': {}, 'suggestions': {'span_label': {'value': [{'label': 'person', 'start': 0, 'end': 2}, {'label': 'person', 'start': 7, 'end': 13}], 'score': None, 'agent': None}}, 'responses': {'span_label': [{'value': [{'label': 'person', 'start': 0, 'end': 2}, {'label': 'person', 'start': 7, 'end': 13}], 'user_id': 'ff683590-024e-4424-acef-de69dcd4f186'}]}, 'vectors': {}, 'status': 'completed', '_server_id': 'bbd777f9-5e71-443b-82e9-91bc36379818'}\n",
      "{'span_label': [{'value': [{'label': 'person', 'start': 0, 'end': 2}, {'label': 'person', 'start': 7, 'end': 13}], 'user_id': 'ff683590-024e-4424-acef-de69dcd4f186'}]}\n",
      "{'id': '18161e4d-9c97-45fb-b096-45116e24c449', 'fields': {'text': 'Go-Forth Pest Control offers services that are not only catered into solving issues such as pest infestations, but also to provide preventive maintenance and servicing work to homes and buildings.'}, 'metadata': {}, 'suggestions': {'span_label': {'value': [], 'score': None, 'agent': None}}, 'responses': {'span_label': [{'value': [{'label': 'organization', 'start': 0, 'end': 21}], 'user_id': 'ff683590-024e-4424-acef-de69dcd4f186'}]}, 'vectors': {}, 'status': 'completed', '_server_id': '0df4acff-e31f-4e00-8fec-f5db4c41e912'}\n",
      "{'span_label': [{'value': [{'label': 'organization', 'start': 0, 'end': 21}], 'user_id': 'ff683590-024e-4424-acef-de69dcd4f186'}]}\n"
     ]
    }
   ],
   "source": [
    "adl = argilla_dataset.records.to_list()\n",
    "\n",
    "for i in adl:\n",
    "    if i['responses']:\n",
    "        print(i)\n",
    "        print(i['responses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Annotated Argilla Dataset\n",
    "\n",
    "GLiNER models return the character indices of the detected entities. For fine-tuning, we need the token indices. So we have to do some data gymnastics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_token_indices(text, tokens, entities):\n",
    "    \"\"\"\n",
    "    Convert character span indices in entities to token indices.\n",
    "    \n",
    "    Args:\n",
    "    - text: The original text as a single string.\n",
    "    - tokens: A list of tokens.\n",
    "    - entities: A list of entities with character start and end indices.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of entities with token start and end indices.\n",
    "    \"\"\"\n",
    "    # Calculate the character start index of each token\n",
    "    token_char_spans = []\n",
    "    current_char_index = 0\n",
    "    for token in tokens:\n",
    "        start_index = text.find(token, current_char_index)\n",
    "        end_index = start_index + len(token)\n",
    "        token_char_spans.append((start_index, end_index))\n",
    "        current_char_index = end_index\n",
    "\n",
    "    # Convert character indices to token indices for each entity\n",
    "    converted_entities = []\n",
    "    for entity in entities:\n",
    "        entity_start, entity_end = entity['start'], entity['end']\n",
    "        entity_start_token = None\n",
    "        entity_end_token = None\n",
    "        \n",
    "        # Find the tokens that the entity start and end indices fall into\n",
    "        for i, (start, end) in enumerate(token_char_spans):\n",
    "            if start <= entity_start < end:\n",
    "                entity_start_token = i\n",
    "            if start < entity_end <= end:\n",
    "                entity_end_token = i + 1\n",
    "                break  # Stop looking once we've found the end token\n",
    "        \n",
    "        if entity_start_token is not None and entity_end_token is not None:\n",
    "            converted_entities.append([entity_start_token, entity_end_token, entity['label']])\n",
    "        else:\n",
    "            print('Error on entity:', entity, 'Tokens:', tokens)\n",
    "    \n",
    "    return converted_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'argilla' has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[223], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m OUTPUT_ROOT\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# exported dataset after review\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m dataset_rg \u001b[38;5;241m=\u001b[39m \u001b[43mrg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m(dataset_name)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# export your Argilla Dataset to a datasets Dataset\u001b[39;00m\n\u001b[1;32m     10\u001b[0m dataset_ds \u001b[38;5;241m=\u001b[39m dataset_rg\u001b[38;5;241m.\u001b[39mto_datasets()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'argilla' has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "OUTPUT_ROOT = Path('data/')\n",
    "OUTPUT_ROOT.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# exported dataset after review\n",
    "dataset_rg = rg.load(dataset_name)\n",
    "\n",
    "# export your Argilla Dataset to a datasets Dataset\n",
    "dataset_ds = dataset_rg.to_datasets()\n",
    "\n",
    "'''\n",
    "Dataset({\n",
    "    features: ['text', 'tokens', 'prediction', 'prediction_agent', 'annotation', 'annotation_agent', 'vectors', 'id', 'metadata', 'status', 'event_timestamp', 'metrics'],\n",
    "    num_rows: 200\n",
    "})\n",
    "'''\n",
    "\n",
    "# format the dataset to GLiNER training format {'tokenized_text' [], 'ner': [ [start_token_i, end_token_i, label], ...], ...}\n",
    "\n",
    "# if it's been annotated, it goes to the evaluation set\n",
    "train_set = []\n",
    "eval_set = []\n",
    "for record in dataset_ds:\n",
    "    \n",
    "    converted_entities = char_to_token_indices(record['text'], record['tokens'], record['annotation'] or record['prediction'])\n",
    "\n",
    "    if record['annotation'] is not None:\n",
    "\n",
    "        eval_set.append(\n",
    "            {\n",
    "                'tokenized_text': record['tokens'], \n",
    "                'ner': converted_entities\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # otherwise, it goes to the weakly annotated train set\n",
    "    else:\n",
    "        train_set.append(\n",
    "            {\n",
    "                'tokenized_text': record['tokens'], \n",
    "                'ner': converted_entities\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "file_path = OUTPUT_ROOT / f\"{dataset_name}_train.jsonl\"\n",
    "with open(file_path, 'w') as file:\n",
    "    for entry in train_set:\n",
    "        json.dump(entry, file)\n",
    "        file.write('\\n')\n",
    "\n",
    "file_path = OUTPUT_ROOT / f\"{dataset_name}_eval.jsonl\"\n",
    "with open(file_path, 'w') as file:\n",
    "    for entry in eval_set:\n",
    "        json.dump(entry, file)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\"tokenized_text\": [\"A\", \"portable\", \"bridge\", \"had\", \"been\", \"prepared\", \"for\", \"crossing\", \"the\", \"canals\", \"which\", \"intersected\", \"the\", \"causeway\", \";\", \"the\", \"intention\", \"being\", \"that\", \"it\", \"should\", \"be\", \"laid\", \"across\", \"a\", \"canal\", \",\", \"that\", \"the\", \"army\", \"should\", \"pass\", \"over\", \"it\", \",\", \"and\", \"that\", \"it\", \"should\", \"then\", \"be\", \"carried\", \"forward\", \"to\", \"the\", \"next\", \"gap\", \"in\", \"the\", \"causeway\", \".\", \"This\", \"was\", \"a\", \"most\", \"faulty\", \"arrangement\", \",\", \"necessitating\", \"frequent\", \"and\", \"long\", \"delays\", \",\", \"and\", \"entailing\", \"almost\", \"certain\", \"disaster\", \".\", \"Had\", \"three\", \"such\", \"portable\", \"bridges\", \"been\", \"constructed\", \",\", \"the\", \"column\", \"could\", \"have\", \"crossed\", \"the\", \"causeway\", \"with\", \"comparatively\", \"little\", \"risk\", \";\", \"and\", \"there\", \"was\", \"no\", \"reason\", \"why\", \"these\", \"bridges\", \"should\", \"not\", \"have\", \"been\", \"constructed\", \",\", \"as\", \"they\", \"could\", \"have\", \"been\", \"carried\", \",\", \"without\", \"difficulty\", \",\", \"by\", \"the\", \"Tlascalans\", \".\", \"At\", \"midnight\", \"the\", \"troops\", \"were\", \"in\", \"readiness\", \"for\", \"the\", \"march\", \".\", \"Mass\", \"was\", \"performed\", \"by\", \"Father\", \"Olmedo\", \";\", \"and\", \"at\", \"one\", \"o'clock\", \"on\", \"July\", \"1st\", \",\", \"1520\", \",\", \"the\", \"Spaniards\", \"sallied\", \"out\", \"from\", \"the\", \"fortress\", \"that\", \"they\", \"had\", \"so\", \"stoutly\", \"defended\", \".\", \"Silence\", \"reigned\", \"in\", \"the\", \"city\", \".\", \"As\", \"noiselessly\", \"as\", \"possible\", \",\", \"the\", \"troops\", \"made\", \"their\", \"way\", \"down\", \"the\", \"broad\", \"street\", \",\", \"expecting\", \"every\", \"moment\", \"to\", \"be\", \"attacked\", \";\", \"but\", \"even\", \"the\", \"tramping\", \"of\", \"the\", \"horses\", \",\", \"and\", \"the\", \"rumbling\", \"of\", \"the\", \"baggage\", \"wagons\", \"and\", \"artillery\", \"did\", \"not\", \"awake\", \"the\", \"sleeping\", \"Mexicans\", \",\", \"and\", \"the\", \"head\", \"of\", \"the\", \"column\", \"arrived\", \"at\", \"the\", \"head\", \"of\", \"the\", \"causeway\", \"before\", \"they\", \"were\", \"discovered\", \".\"], \"ner\": [[29, 30, \"organization\"], [116, 117, \"organization\"], [121, 122, \"organization\"], [133, 135, \"person\"], [147, 148, \"organization\"], [210, 211, \"person\"], [217, 218, \"organization\"]]}\n",
    "data = train_set[10]\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span, Doc\n",
    "\n",
    "# Create a Doc from the tokenized text\n",
    "doc = Doc(nlp.vocab, words=data[\"tokenized_text\"])\n",
    "ents = []\n",
    "for start, end, label in data[\"ner\"]:\n",
    "    span = Span(doc, start, end, label=label)\n",
    "    ents.append(span)\n",
    "doc.ents = ents\n",
    "\n",
    "# Visualize the NER entities\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKuSFsW8hIlI"
   },
   "source": [
    "## Appendix: Log datasets to the Hugging Face Hub\n",
    "\n",
    "Here we will show you an example of how you can push an Argilla dataset (records) to the [Hugging Face Hub](https://huggingface.co/datasets).\n",
    "In this way, you can effectively version any of your Argilla datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7P0CEANhIlI"
   },
   "outputs": [],
   "source": [
    "# records = rg.load(dataset_name)\n",
    "# records.to_datasets().push_to_hub(\"<name of the dataset on the HF Hub>\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "zero-shot-ner-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "metadata": {
   "interpreter": {
    "hash": "0f338a8622467eba0ef87b9a79c52cc260cef0b0d60c3c739596fb787bf801dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
